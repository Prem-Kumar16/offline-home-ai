{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "637a1430-84fe-4dd1-aaaa-91ec9baa7652",
   "metadata": {},
   "source": [
    "## Load the trained LoRA and merge into full weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f5e0fe22-166b-45a2-ae4d-0fba3efe7b08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./tinyllama-1.1b-merged/tokenizer_config.json',\n",
       " './tinyllama-1.1b-merged/special_tokens_map.json',\n",
       " './tinyllama-1.1b-merged/chat_template.jinja',\n",
       " './tinyllama-1.1b-merged/tokenizer.json')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import PeftModel\n",
    "import torch\n",
    "import os\n",
    "\n",
    "base_model_id = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "lora_dir = \"./tinyllama-1.1b-lora-final\"  # your saved adapter dir\n",
    "\n",
    "# Load base model (FP16/BF16)\n",
    "base = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_id,\n",
    "    torch_dtype=torch.float16,  # or bfloat16 if GPU supports\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "# Attach LoRA\n",
    "peft_model = PeftModel.from_pretrained(base, lora_dir)\n",
    "\n",
    "# Merge LoRA into base weights and unload adapters\n",
    "merged = peft_model.merge_and_unload()  # produces a standard HF model\n",
    "\n",
    "# Save merged model in HF format\n",
    "out_dir = \"./tinyllama-1.1b-merged\"\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "merged.save_pretrained(out_dir, safe_serialization=True)  # safetensors\n",
    "tok = AutoTokenizer.from_pretrained(base_model_id, use_fast=True)\n",
    "tok.save_pretrained(out_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8e660b6-9c3e-4ca2-bd88-8a47ad718c2a",
   "metadata": {},
   "source": [
    "## Convert merged HF model to GGUF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29562fe7-da67-4b49-8bbe-1bb0bd3d5dda",
   "metadata": {},
   "source": [
    "Use llama.cppâ€™s converter and quantizer:\n",
    "\n",
    "1. Clone and build llama.cpp, install requirements:\n",
    "\n",
    "* git clone https://github.com/ggerganov/llama.cpp\n",
    "* cd llama.cpp && pip install -r requirements.txt\n",
    "* make (optional; quantize binary is built by make)\n",
    "\n",
    "2. Convert HF to GGUF (F16):\n",
    "\n",
    "* python3 llama.cpp/convert_hf_to_gguf.py /home/ubuntu/HA-Assist/tinyllama-1.1b-merged\n",
    "* This produces ggml-model-f16.gguf in the merged folder."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "503e7d84-b33e-4f12-b0a1-df831b58b9b5",
   "metadata": {},
   "source": [
    "## Quantize to Q4_K_M"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b459e09-6a3a-4c0e-b697-c863fb9e3709",
   "metadata": {},
   "source": [
    "* Q4_K_M is supported and commonly used with Ollama.\n",
    "* ./build/bin/llama-quantize /home/ubuntu/HA-Assist/tinyllama-1.1b-merged/tinyllama-1.1B-merged-F16.gguf /home/ubuntu/HA-Assist/tinyllama-1.1b-merged/tinyllama-1.1B-merged-Q4_K_M.gguf Q4_K_M"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f696a217-2297-4461-83f8-346152ced74d",
   "metadata": {},
   "source": [
    "## Preparing Hugging Face repo contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "93785a6c-220b-4b5f-8158-86b72dcc5da3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[90mgit version 2.34.1\u001b[0m\n",
      "\u001b[1m\u001b[31mLooks like you do not have git-lfs installed, please install. You can install from https://git-lfs.github.com/. Then run `git lfs install` (you only have to do this once).\u001b[0m\n",
      "\n",
      "You are about to create \u001b[1mpremrajreddy/Home-TinyLlama-1.1B-HomeAssist-GGUF\u001b[0m\n",
      "\n",
      "Your repo now lives at:\n",
      "  \u001b[1mhttps://huggingface.co/premrajreddy/Home-TinyLlama-1.1B-HomeAssist-GGUF\u001b[0m\n",
      "\n",
      "You can clone it locally with the command below, and commit/push as usual.\n",
      "\n",
      "  git clone https://huggingface.co/premrajreddy/Home-TinyLlama-1.1B-HomeAssist-GGUF\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!huggingface-cli repo create Home-TinyLlama-1.1B-HomeAssist-GGUF --type model -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "03edbcde-3a7b-4b85-8984-45f3afb6840e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e11fd4db1e334a20a9ecb402eef67811",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tinyllama-1.1B-merged-Q5_K_M.gguf:   0%|          | 0.00/782M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc6eb54c82a1406c9fb956fafa6bb36c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tinyllama-1.1B-merged-Q4_1.gguf:   0%|          | 0.00/701M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f692ab0b25344b8caafd058be668b7db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload 4 LFS files:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e0f16c9f2604a3f8c1d19d97121e56e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tinyllama-1.1B-merged-Q4_0.gguf:   0%|          | 0.00/637M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d56c9ca1b9a43f696ae26f807456f44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tinyllama-1.1B-merged-Q8_0.gguf:   0%|          | 0.00/1.17G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/premrajreddy/Home-TinyLlama-1.1B-HomeAssist-GGUF/commit/ff55257e2e830e8c36acc5c757c34d67ee81bd04', commit_message='Upload folder using huggingface_hub', commit_description='', oid='ff55257e2e830e8c36acc5c757c34d67ee81bd04', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from huggingface_hub import HfApi\n",
    "api = HfApi()\n",
    "\n",
    "api.upload_folder(\n",
    "    folder_path=\"./tinyllama-1.1b-merged\",\n",
    "    repo_id=\"premrajreddy/Home-TinyLlama-1.1B-HomeAssist-GGUF\",\n",
    "    repo_type=\"model\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0769a735-801b-4162-8a08-8d7d84943f36",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
